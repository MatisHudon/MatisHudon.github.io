[{"authors":["[R Monroy](http://www.rmonroy.com/), M Hudon, [A Smolić](https://www.tcd.ie/research/profiles/?profile=smolica)"],"categories":null,"content":"","date":1537488000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537488000,"objectID":"59d0c0a22a43db52f1f3d3785cb7d92e","permalink":"https://matishudon.github.io/publication/dynamic-environment-mapping-for-augmented-reality-applications-on-mobile-devices/","publishdate":"2018-09-21T00:00:00Z","relpermalink":"/publication/dynamic-environment-mapping-for-augmented-reality-applications-on-mobile-devices/","section":"publication","summary":"Augmented Reality is a topic of foremost interest nowadays. Its main goal is to seamlessly blend virtual content in real-world scenes. Due to the lack of computational power in mobile devices, rendering a virtual object with high-quality, coherent appearance and in real-time, remains an area of active research. In this work, we present a novel pipeline that allows for coupled environment acquisition and virtual object rendering on a mobile device equipped with a depth sensor. While keeping human interaction to a minimum, our system can scan a real scene and project it onto a two-dimensional environment map containing RGB+Depth data. Furthermore, we define a set of criteria that allows for an adaptive update of the environment map to account for dynamic changes in the scene. Then, under the assumption of diffuse surfaces and distant illumination, our method exploits an analytic expression for the irradiance in terms of spherical harmonic coefficients, which leads to a very efficient rendering algorithm. We show that all the processes in our pipeline can be executed while maintaining an average frame rate of 31Hz on a mobile device.","tags":null,"title":"Dynamic Environment Mapping for Augmented Reality Applications on Mobile Devices","type":"publication"},{"authors":["M Hudon, [R Pagés](http://rafapages.com/), [M Grogan](https://www.scss.tcd.ie/~groganma/), [A Smolić](https://www.tcd.ie/research/profiles/?profile=smolica)"],"categories":null,"content":"","date":1536710400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536710400,"objectID":"3885523729d9d25fcf6f2f614c8a46ae","permalink":"https://matishudon.github.io/publication/deep-normal-estimation-for-automatic-shading-of-hand-drawn-characters/","publishdate":"2018-09-12T00:00:00Z","relpermalink":"/publication/deep-normal-estimation-for-automatic-shading-of-hand-drawn-characters/","section":"publication","summary":"We present a new fully automatic pipeline for generating shading effects on hand-drawn characters. Our method takes as input a single digitized sketch of any resolution and outputs a dense normal map estimation suitable for rendering without requiring any humna input. At the heart of our method lies a deep residual, encoder-decoder convolutional network. The input sketch is first sampled using several equally sized 3-channel windows, with each window capturing a local area of interest at 3 different scales. Each window is then passed through the previously trained network for normal estimation. Finally, network outputs are arranged together to form a full-size normal map of the input sketch. We also present an efficient and effective way to generate a rich set of training data. Resulting renders offer a rich quality without any effort from the 2D artist. We show both quantitative and qualitative results demonstrating the effectiveness and quality of our network and method.","tags":null,"title":"Deep Normal Estimation for Automatic Shading of Hand-Drawn Characters","type":"publication"},{"authors":["[M Grogan](https://www.scss.tcd.ie/~groganma/), M Hudon, Daniel McCormack, [A Smolić](https://www.tcd.ie/research/profiles/?profile=smolica)"],"categories":null,"content":"","date":1535500800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535500800,"objectID":"97f98688789c91435ec4846e7982e710","permalink":"https://matishudon.github.io/publication/automatic-palette-extraction-for-image-editing/","publishdate":"2018-08-29T00:00:00Z","relpermalink":"/publication/automatic-palette-extraction-for-image-editing/","section":"publication","summary":"Interactive palette based colour editing applications have grown in popularity in recent years, but while many methods propose fast palette extraction techniques, they typically rely on the user to define the number of colours needed. In this paper, we present an approach that extracts a small set of representative colours from an image automatically, determining the optimal palette size without user interaction. Our iterative technique assigns a vote to each pixel in the image based on how close they are in colour space to the colours already in the palette. We use a histogram to divide the colours into bins and determine which colour occurs most frequently in the image but is far away from all of the palette colours, and we add this colour to the palette. This process continues until all pixels in the image are well represented by the palette. Comparisons with existing methods show that our colour palettes compare well to other state of the art techniques, while also computing the optimal number of colours automatically at interactive speeds. In addition, we showcase how our colour palette performs when used in image editing applications such as colour transfer and layer decomposition.","tags":null,"title":"Automatic Palette Extraction for Image Editing","type":"publication"},{"authors":["M Hudon, [R Pagés](http://rafapages.com/), [M Grogan](https://www.scss.tcd.ie/~groganma/), [J Ondřej](https://www.scss.tcd.ie/Jan.Ondrej/), [A Smolić](https://www.tcd.ie/research/profiles/?profile=smolica)"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"89d82066144ae10322b603337eb43cc4","permalink":"https://matishudon.github.io/publication/2d-shading-for-cel-animation/","publishdate":"2018-06-01T00:00:00Z","relpermalink":"/publication/2d-shading-for-cel-animation/","section":"publication","summary":"We present a semi-automatic method for creating shades and self-shadows in cel animation. Besides producing attractive images, shades and shadows provide important visual cues about depth, shapes, movement and lighting of the scene. In conventional cel animation, shades and shadows are drawn by hand. As opposed to previous approaches, this method does not rely on a complex 3D reconstruction of the scene: its key advantages are simplicity and ease of use. The tool was designed to stay as close as possible to the natural 2D creative environment and therefore provides an intuitive and user-friendly interface. Our system creates shading based on hand-drawn objects or characters, given very limited guidance from the user. The method employs simple yet very efficient algorithms to create shading directly out of drawn strokes. We evaluate our system through a subjective user study and provide qualitative comparison of our method versus existing professional tools and state of the art. ","tags":null,"title":"2D Shading for Cel Animation","type":"publication"},{"authors":["M Hudon, [R Pagés](http://rafapages.com/), [M Grogan](https://www.scss.tcd.ie/~groganma/), [J Ondřej](https://www.scss.tcd.ie/Jan.Ondrej/), [A Smolić](https://www.tcd.ie/research/profiles/?profile=smolica)"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"f937b23c7317c1005f5bade7e691e029","permalink":"https://matishudon.github.io/publication/2dtoonshade_a_stroke_based_toon_shading_system/","publishdate":"2018-06-01T00:00:00Z","relpermalink":"/publication/2dtoonshade_a_stroke_based_toon_shading_system/","section":"publication","summary":"We present 2DToonShade: a semi-automatic method for creating shades and self-shadows in cel ani- mation. Besides producing attractive images, shades and shadows provide important visual cues about depth, shapes, movement and lighting of the scene. In conventional cel animation, shades and shadows are drawn by hand. As opposed to previous approaches, this method does not rely on a complex 3D re- construction of the scene: its key advantages are simplicity and ease of use. The tool was designed to stay as close as possible to the natural 2D creative environment and therefore provides an intuitive and user-friendly interface. Our system creates shading based on hand-drawn objects or characters, given very limited guidance from the user. The method employs simple yet very efficient algorithms to create shad- ing directly out of drawn strokes. We evaluate our system through a subjective user study and provide qualitative comparison of our method versus existing professional tools and recent state of the art.","tags":null,"title":"2DToonShade: A stroke based toon shading system","type":"publication"},{"authors":["M Hudon","[R Cozot](http://cozot.free.fr/)","[K Bouatouch](http://www.irisa.fr/frvsense/Kadi.Bouatouch/)"],"categories":null,"content":"","date":1464739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464739200,"objectID":"0767e63e5a99e31dad53cb9e00b2f0e3","permalink":"https://matishudon.github.io/publication/automatic-light-compositing/","publishdate":"2016-06-01T00:00:00Z","relpermalink":"/publication/automatic-light-compositing/","section":"publication","summary":"Lighting is a key element in photography. Professional photographers often work with complex lighting setups to directly capture an image close to the targeted one. Some photographers reversed this traditional workflow. Indeed, they capture the scene under several lighting conditions, then combine the captured images to get the expected one. Acquiring such a set of images is a tedious task and combining them requires some skills in photography. We propose a fully automatic method, that renders, based on a 3D reconstructed model (shape and albedo), a set of images corresponding to several lighting conditions. The resulting images are combined using a genetic optimization algorithm to match the desired lighting provided by the user asan image","tags":null,"title":"Automatic Light Compositing using Rendered Images","type":"publication"},{"authors":["M Hudon, [A Gruson](https://beltegeuse.github.io/research/), [P Kerbiriou](http://www.technicolor.com/en/paul-kerbiriou), [R Cozot](http://cozot.free.fr/), [K Bouatouch](http://www.irisa.fr/frvsense/Kadi.Bouatouch/)"],"categories":null,"content":"","date":1454284800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1454284800,"objectID":"eed9fd1112878d3d03be3427160a7569","permalink":"https://matishudon.github.io/publication/shape-and-reflectance/","publishdate":"2016-02-01T00:00:00Z","relpermalink":"/publication/shape-and-reflectance/","section":"publication","summary":"In this paper we propose a method for recovering the shape (geometry) and the diffuse reflectance from an image (or video) using a hybrid setup consisting of a depth sensor (Kinect), a consumer camera and a partially controlled illumination (using a flash). The objective is to show how combining RGB-D acquisition with a sequential illumination is useful for shape and reflectance recovery. A pair of two images are captured: one non flashed (image under ambient illumination) and a flashed one. A pure flash image is computed by subtracting the non flashed image from the flashed image. We propose an novel and near real-time algorithm, based on a local illumination model of our flash and the pure flash image, to enhance geometry (from the noisy depth map) and recover reflectance information.","tags":null,"title":"Shape and Reflectance from RGB-D Images using Time Sequential Illumination","type":"publication"},{"authors":["M Hudon, [P Kerbiriou](http://www.technicolor.com/en/paul-kerbiriou), [A Schubert](http://www.technicolor.com/en/arno-schubert), [K Bouatouch](http://www.irisa.fr/frvsense/Kadi.Bouatouch/)"],"categories":null,"content":"","date":1433116800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1433116800,"objectID":"027c830440b9c566e044dd57e6db1ad1","permalink":"https://matishudon.github.io/publication/high-speed-sequential/","publishdate":"2015-06-01T00:00:00Z","relpermalink":"/publication/high-speed-sequential/","section":"publication","summary":"Nowadays flashes are commonly used in photography: they bring light and sharpness to images. It is very tempting to use flash lights in cinema, to take profit of controlled light as photographers may do. But using flashes with video recording is not as easy as in photography. Actually, flashes cause many temporal artifacts in video recordings, especially with high speed CMOS cameras equipped with electronic rolling shutters. This paper proposes a video recording method that uses periodic strobbed illumination sources together with any electronic rolling shutter camera, even without any synchronization device between the camera and the controlled lights. The objective is to avoid recording artifacts by controlling the timings and periods of the flash lights, and then reconstructing images using rows that correspond to the same flash instant. We will show that our method can be easily applied to photometric stereo.","tags":null,"title":"High speed sequential illumination with electronic rolling shutter cameras","type":"publication"}]